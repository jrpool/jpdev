<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

<title>Interlingual Annotation for Machine Translation</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />

<link rel="stylesheet" type="text/css" href="styles.css" title="styles">

</head>

<body>

<table><tr>

<td class="nav">
<p><a href="index.html">Start</a></p>
<p><a href="intro.html">Introduction</a></p>
<p><a href="proj.html">Project</a></p>
<p><a href="corp.html">Corpora</a></p>
<p><a href="il.html">Interlingua</a></p>
<p><a href="tools.html">Tools</a></p>
<p><a href="proc.html">Process</a></p>
<p class="this">Evaluation</p>
<p><a href="issues.html">Issues</a></p>
<p><a href="eval.html"><img src="/images/prior.gif" /></a><a href="eval-howex.html"><img src="/images/next.gif" /></a></p>
</td>

<td class="main">
<h3>How was inter-annotator agreement measured?</h3>
<h4>Measurement context</h4>
<p>IL0-to-IL1 annotation.</p>
<p>10 annotators.</p>
<p>Each annotator annotated 72 texts (6 languages, 6 articles per language, 2 translations per article).</p>
<p>Each text contained about 250 words, so 72 texts contained about 18,000 words.</p>
<p>Of the 18,000 words, 1,268 (7%) were to be annotated. These were the nouns, verbs, adjectives, and adverbs.</p>
<h4>Explicit agreement measure</h4>
<p>Consider any pair of annotators and any token in the corpus.</p>
<p>Call each annotator's "total count" the count of categories assigned to the token.</p>
<p>Call each annotator's "shared count" the count of the categories in that annotator's assignment to the token that were also in the other annotator's assignment to the token.</p>
<p>Then the agreement of that pair of annotators on that token is the sum of the 2 shared counts divided by the sum of the 2 total counts.</p>
<p>The agreement of that pair of annotators on the corpus is the mean of the pair's agreements on all the tokens.</p>
<p>The explicit agreement measure is the mean of all pairs' agreements on the corpus.</p>
<h4>Implicit agreement measure</h4>
<p>More cryptically described. Apparently, the fraction of all categories (e.g., 110,000 word senses) on which an annotator pair's members both assign or both fail to assign to a word. Thus, if you assign only "buffalo" and I assign only "whisky" to "bank" and there are 110,000 senses, our agreement on "bank" is 109,998/110,000.</p>
<h4>Derived measures</h4>
<p>Kappa statistic derived from each agreement measure.</p>
</td>

</tr></table>

</body>
</html>
