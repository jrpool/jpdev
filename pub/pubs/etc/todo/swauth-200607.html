<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

<title>Mass Semantic Authoring</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />

<link rel="stylesheet" type="text/css" href="styles.css" title="styles" />

</head>

<body style="margin: 30pt; background-color: #EEEEDD">

<h2>Mass Semantic Authoring</h2>

<h3><b>Jonathan Pool</b></h3>
<h4 class="c">University of Washington<br />
Turing Center<br />
July, 2006</h4>

<h4>Contents</h4>
<p class="l"><a href="#ques">The Question</a></p>
<p class="l"><a href="#known">Current Knowledge</a></p>
<p class="l"><a href="#cont">Contribution</a></p>
<p class="l"><a href="#pilot">Pilot Study</a></p>
<p class="l"><a href="#main">Main Study</a></p>
<p class="l"><a href="#pubs">Publications</a></p>
<p class="l"><a href="#req">Requirements</a></p>
<p class="l"><a href="#refs">References</a></p>

<h3><a id="ques">The Question</a></h3>

<p>How easily can Web authors formally represent meanings?</p>

<p>The Semantic Web is one of the most influential ideas about the world's information future, but there is debate about its feasibility, and particularly about the feasibility of mass authorship of Semantic Web documents.</p>

<p>Proponents assert that Web authors without knowledge-representation expertise could use "off-the-shelf software for writing Semantic Web pages" (i.e. for representing page content as RDF using OWL ontologies), implying that such software would make the writing task easy enough to motivate ordinary Web authors to represent formally the meanings they have in mind, even when authors need to define their own terms to represent concepts for which no terms exist yet (<cite>Berners-Lee 2001</cite>).</p>

<p>Skeptics respond that user-friendly and powerful software could spare human Semantic Web authors the tedium of authorship management, but could not obviate the need to use a formal representation language and formally defined concepts and learn how representations affect the results produced by reasoning algorithms. All this "requires the author to become a skilled knowledge engineer", and most Web authors would not be willing to undergo enough training to acquire this skill (<cite>Marshall 2003</cite>).</p>

<p>Even skeptics, however, say that the formally represented fraction of the content of the Web is "increasing dramatically" (<cite>Shirky 2003</cite>), though in a bottom-up way that does not respect the ontological and syntactic standardization envisioned for the Semantic Web. They forecast a highly annotated Web, produced and consumed partly by machines, one less coherent than the envisioned Semantic Web but already in progress. Whether coherently or not, the Web seems, then, on the way to becoming rich in semantically formalized content and rich also in automated agents reasoning with such content.</p>

<p>Given this trend, human authors of at least some content will benefit by formally representing meanings, but how easily can they do so?</p>

<p>This question has stronger and weaker versions, corresponding to stronger and weaker visions of a Semantic Web. Does the vision require creative authors, or only descriptive authors? Creative authors would design their own ontologies when existing ones fail to represent the meanings to be communicated; descriptive authors would merely use existing ontologies, which would be assumed to be satisfactory. Does the vision require authors to represent, or only to classify? Authors who represent would restate the content; authors who classify would merely assign categories to the documents. The strongest vision asserts that a Semantic Web with creative, representing authors is feasible. The weakest vision asserts that a Semantic Web with descriptive, classifying authors is feasible.</p>

<h3><a id="known">Current Knowledge</a></h3>

<p>A cursory search has failed to discover any rigorously supported knowledge about the difficulty of formal semantic authoring. There is much published work on semantic models and formalisms for meaning representation, and there are many tools providing user interfaces to them, but there are no studies of how easily people can work with these models, formalisms, and tools. Almost all current work that is classified as Semantic Web research ignores author usability, expresses awareness of usability obstacles but fails to measure them, or simply assumes (contrary to the Semantic Web vision) that the formal representation of meanings is too difficult for the authors or consumers of Web content.</p>

<p>For example, the Dublin Core Metadata Initiative (DCMI, <cite><a href="http://www.dublincore.org/">http://www.dublincore.org/</a></cite>) is producing a concept taxonomy for resource description, suitable (it hopes) for use by any creator of informational documents. DCMI seeks to get its vocabulary multilingualized and multiculturalized and to promote the development of software tools to support its use by authors, but there is only anecdotal knowledge (e.g., <cite><a href="http://www.lib.helsinki.fi/meta/#Tasks">http://www.lib.helsinki.fi/meta/#Tasks</a></cite>) about how easily people can use this standard. Committee members deliberating on standardization decisions can only speculate about user acceptability, as in this case:</p>

<blockquote>
<p class="l">Well, I continue to assert that the values of educationLevel are "classes of entities" and that when I assert an instance of the property to be "grade 5", I am meaning the category of students (AgentGroup) that are situated (in the US and Canada) at a certain point in their progressing through the "system". That is why it was conceived as a subproperty of audience and not some new property. However, I am afraid that changing the definition will probably not help at all in changing practice in the real world. <cite><a href="http://dublincore.org/usage/meetings/2006/04/seattle/2006-04-29.ub-agenda-seattle.pdf">http://dublincore.org/usage/meetings/2006/04/seattle/2006-04-29.ub-agenda-seattle.pdf</a>, p. 16</cite></p>
</blockquote>

<p>There is also no systematic empirical evidence on the much-discussed question whether formalisms that resemble natural languages are easier to use than machine-oriented formalisms, or more generally on the usability of natural-language-like formalisms (M&oslash;ller 2003, p. 1). There are positive and negative impressionistic observations about this. A study on KANT, a controlled variety of English, notes that "it can be difficult for an author to determine how to rewrite an existing sentence to conform to the rules of controlled language" (<cite>Mitamura 2003, p. 1</cite>). By contrast, according to a study on Attempto Controlled English (ACE), "learning a controlled language is much easier than learning logic, and takes only a couple of days for the basics and 4-6 weeks for full proficiency" (<cite>Bernstein 2004, p. 5</cite>). Another study comments, "the commercial success of some controlled languages, e.g., AECMA simplified English, suggests that people can indeed learn to work with restricted English" (<cite>Clark 2005, p. 1</cite>). And yet the same study, reporting on a controlled English named CPL, cautions that</p>

<blockquote>
<p class="l">it still takes some skill to use CPL. ... the user needs to learn to "control the beast" to ensure that what the system understands is what he/she intended, and modify the input and/or processing if not. It may take several attempts to enter a rule into the system until it is understood correctly (users familiar with CPL and the interpreter are significantly more efficient). ... In particular, a "natural" statement of a rule often leaves much knowledge implicit, which needs to be made explicit to produce a meaningful and useful rule in CPL. ... [T]he reformulation task is often more than just rephrasing; it requires making the "natural" language more precise, which in turn requires thinking in a KR-oriented way about the subject matter being described. This itself requires a certain amount of skill and training. ... To correct misinterpretations, the user needs to know a "bag of tricks" for rephrasing the input to avoid specific mistakes. ... A challenge for languages like CPL is to devise methods so that these corrective strategies are taught to the user at just the right time, e.g., through the use of good system feedback and problem-specific on-line help. <cite>Clark 2005, p. 5</cite></p>
</blockquote>

<p class="l">One study (M&oslash;ller 2003) found ASD Simplified Technical English to be less acceptable to readers than natural English, but did not investigate its usability for writers.</p>

<p>The Open University's Knowledge Management Institute describes 11 current Semantic Web projects (<cite><a href="http://kmi.open.ac.uk/technologies/hot.cfm?section=swks">http://kmi.open.ac.uk/technologies/hot.cfm?section=swks</a></cite>), focused on developing environments for the support of content creators and consumers, but none of them involves people formally representing meanings. One project is explained with the observation that "modern ontological markup, though powerful, can be onerous". The projects mainly attempt to induce formal meanings automatically from natural-language content or queries.</p>

<p>A 2005 conference, "Workshop on User Aspects of the Semantic Web" (<cite><a href="http://kmi.open.ac.uk/events/usersweb/">http://kmi.open.ac.uk/events/usersweb/<a></cite>), aimed "to look at how an 'ordinary user' might be able to tap into the resources of the Semantic Web, find out about the value of these resources for their work practice or their general web use, and feel compelled to use and perhaps even contribute to Semantic Web resources." The proceedings (<cite><a href="http://CEUR-WS.org/Vol-137/">http://CEUR-WS.org/Vol-137/<a></cite>) include 15 reports, of which 5 deal at least partly with semantic authoring. Koivunen (pp. 5-18) describes the Annotea system for browser-based user annotation of resources, but reports no data on user effort or satisfaction. Takeda and Ohmukai (pp. 43-58) cursorily describe methods of end-user semantic representation of personal schedules, contacts, and Weblog topics, based on a strategy of providing both instant and nearly instant gratification to users (extending Turing Center work by McDowell and others), but their usability data consist only of usage and application-retrieval rates. Fillies et al. (pp. 85-92) describe software for graphical ontology authoring but no data about its usability. Zhdanova (pp. 111-124) describes benefits that community-driven Semantic Web portals with collaborative ontology management could confer, but reports no evidence of usability. W&#x0119;cel and Zhdanova (pp. 161-175) advocate a strategy for the facilitation of end-user semantic representation in both production and consumption of Web data, but report no evaluation.</p>

<p>Many software products have been designed to support formal semantic authoring, but almost nothing is known about their usability. One compilation (<cite><a href="http://planetrdf.com/guide/#sec-tools">http://planetrdf.com/guide/#sec-tools</a></cite>) lists 128 editors and tools for authoring with the RDF formalism, but gives no information about any usability evaluations of them. There is at least one exception: The WYSIWYM direct-manipulation authoring interface for representing meanings in a syntax-derived formalism has been estimated by Power (2003, pp. 6-7) to require about 5 times as much effort as composition in uncontrolled English.</p>

<p>Users' one-time learning costs, recurring use costs, satisfaction levels, and performance successes and failures remain largely unknown.</p>

<h3><a id="cont">Contribution</a></h3>

<p>The research proposed here would produce the first systematic, quantitative evaluation of the effort required by ordinary Web authors for the successful production of formal semantic representations of Web documents. The estimates would include (1) one-time learning efforts, (2) recurrent production efforts, (3) user satisfaction, and (4) production quality. The estimates would permit comparisons with natural-language authoring.</p>

<h3><a id="pilot">Pilot Study</a></h3>

<p>The pilot study seeks to determine whether even the weakest version of the Semantic Web vision is realistic. It investigates the ability of authors to be descriptive classifiers: to classify their documents with existing classification schemes. If the result is positive, further studies will investigate whether creative and/or representing authorship is practical. If the pilot study's result is negative, even the weakest vision of the Semantic Web is unrealistic, and the stronger visions must be even more so; then the cost of investigations of the stronger visions need not be incurred.</p>

<p>The first pilot-study strategy to be attempted is the analysis of existing data. A plausible candidate is the data maintained by the Open Directory Project about URL submissions. An individual suggests a Web site or subsite for inclusion in the Open Directory, specifying its URL and the single category (out of about 600,000) deemed most suitable in the directory's classification scheme. The ODP's editors decide whether to include the suggested URL in the directory and, if so, whether to accept the proposed category or assign the URL to a different category. As a first approximation, those submitting suggestions can be considered authors (when not authors, they presumably have author-like familiarity with the content domain), and the editors making the decisions can be considered competent judges of the correct categories. Thus, the URLs that the editors recategorize, as a fraction of all suggested URLs accepted into the directory, are a first approximation of the rate at which authors fail to classify their documents correctly.</p>

<p>The ODP has a practice of long-term record retention, particularly about editor performance, so it can be surmised that data on reclassification of submitted URLs have not been discarded.</p>

<p>Using ODP data for the pilot study will apparently require obtaining the cooperation of ODP (and perhaps its sponsor, AOL Netscape). ODP publishes a monthly statistical report (<cite><a href="http://research.dmoz.org/publish/chris2001/odp_reports/">http://research.dmoz.org/publish/chris2001/odp_reports/</a></cite>), but it does not contain any data on editorial reclassifications of submitted URLs. ODP is besieged by submitters who complain about waiting for years with no information about action on their submissions (e.g., <cite><a href="http://forums.seochat.com/open-directory-project-13/i-feel-bad-sometimes-for-being-a-dmoz-editor-87048.html">http://forums.seochat.com/open-directory-project-13/i-feel-bad-sometimes-for-being-a-dmoz-editor-87048.html</a></cite>), so it cannot be assumed that access to the data will be straightforward.</p>

<p>If the reclassification data can be obtained, they will need to be inspected for more than a mere reclassified fraction. Questions to address may include the determinants of author error and the causes, other than author error, of editorial reclassifications. The May 2006 ODP statistical report discusses reclassifications, saying "The most frequently repeated complaint is that ODP editors do not follow webmasters' suggestions regarding category, title and description. This is correct: ODP is a directory created for the searchers, no marketing tool, and its guidelines have been written accordingly. So ... listings will be placed in the categories where they fit best and not in the categories which suit best from the marketing standpoint." This suggests that some reclassifications result from adversarial classification by authors rather than from author errors. A likely example of this pattern is submissions of sites that would interest a mainly local audience under universal categories instead of regional ones. Analysis of the data may permit an estimated correction of the reclassified fraction to eliminate the portion due to non-erroneous, adversarial classification. Errors by authors can be analyzed for the discovery of their severities (how poorly authors misjudge the most appropriate category) and kinds (the patterns describing errors that authors make). From these results, hypotheses can be derived as to preventive measures, such as training or checking, that could most efficiently decrease the rate and the severity of author classification errors.</p>

<p>Given these complexities and the lack of experimental control, why analyze the ODP data instead of performing an experimental study? ODP has about 5 million URLs, of which an unknown but presumably large fraction has been classified by submitters and then accepted by human editors with or without reclassification (editors also add URLs to the directory on their own initiative). The volume of the ODP data and its low cost (if available) may justify its analysis prior to studies that require new human classifications.</p>

<p>The proposed steps in the pilot study based on the ODP classification data are:</p>

<ol>
<li>Perform a review of the applicable literature to check for related work and possible prior analyses of the target data.</li>
<li>Determine appropriate persons to contact for an inquiry into the existing data and access to them.</li>
<li>Make the inquiry and negotiate access.</li>
<li>Design and execute the analysis of the data.</li>
</ol>

<p>If access to the ODP data is impossible, alternative existing data may be sought. I have not discovered other sources that seem suitable. One project that may have generated usable data is FAST (<cite><a href="http://www.oclc.org/ca/en/research/projects/fast/default.htm">http://www.oclc.org/ca/en/research/projects/fast/default.htm</a></cite>), sponsored by OCLC. Its purpose is to modify an existing subject-classification system with about 300,000 categories to make it "easier to understand, control, apply, and use". The intended user community includes persons without "sophisticated training in subject indexing and classification" (<cite>Dean 2003, p. 1</cite>). No publications reporting any empirical study of its usability have been found, however.</p>

<p>If no existing data can be analyzed for the pilot study, new data can be produced in a laboratory or field study that ascertains the reliability of multiple subjects' classifications of identical documents. The essential constraint will be author-equivalent classification: classification of documents by their authors or by persons who are as familiar with them as their authors would be. Thus, the elicitation of classifications of randomly selected Web pages or excerpts therefrom, analogous to the word-tagging of randomly presented images in the ESP Game (<cite><a href="http://www.espgame.org/">http://www.espgame.org/</a></cite>), would be inappropriate. Many Web sites would probably be difficult for most individuals to classify correctly (consider <cite><a href="http://www.milkandcookies.com">http://www.milkandcookies.com</a></cite>, for example), but that does not invalidate the weakest, or even the strongest, vision of the Semantic Web, because no vision of the Semantic Web expects typical persons to be able to classify or represent knowledge in domains with which they are unfamiliar. In a study, this constraint increases the cost of finding persons who can classify documents. For applicability to the Turing Center's studies of panlingual translation, the classification scheme for such a study would ideally be one that has multilingual equivalent versions. Among these are the ODP's classification scheme, at least parts of which exist in 78 languages, and the Dewey Decimal Classification, which exists in 30 languages (<cite>Vizine-Goetz 2002, p. 9</cite>).</p>

<h3><a id="main">Main Study</a></h3>

<p>The main study is conducted if the pilot study yields substantial evidence in favor of the practicality of the weakest Semantic Web vision. That result would make it useful to discover how strong a vision is practical, and what conditions maximize the ability of Semantic Web authors to perform their functions.</p>

<p>There are two dimensions along which the main study could impose incrementally greater demands on authors. One is the description-creation dimension. The other is the classification-representation dimension. It seems reasonable to begin increasing the demands along the classification-representation dimension: continuing to confine authors to existing ontologies, but asking them to move from merely classifying their documents by subject to representing the statements contained in the documents.</p>

<p>A possible main study, then, investigates the difficulty for Web authors of formally representing (some of) the meanings that they intend to communicate. This study applies the semantic model that has been adopted by the Semantic Web community. For the authoring of documents that make use of existing ontologies, this is the semantic model of RDF, an intensional "version of existential binary relational logic in which relations are first-class entities in the universe of quantification" (<cite><a href="http://www.w3.org/TR/rdf-mt/">http://www.w3.org/TR/rdf-mt/</a></cite>). (For the authoring of ontologies themselves, the semantic model is the OWL extension of the RDF model (<cite><a href="http://www.w3.org/TR/owl-semantics/">http://www.w3.org/TR/owl-semantics/</a></cite>).)</p>

<p>Given the RDF semantic model, the study experimentally compares two compatible formalisms. One is the RDF representation in XML syntax (RDF/XML), the standard formalism of the Semantic Web initiative (<cite><a href="http://www.w3.org/TR/rdf-primer/">http://www.w3.org/TR/rdf-primer/</a></cite>). The other is a controlled natural language designed to permit the representation of any meaning that can be represented in RDF/XML. The most appropriate controlled natural language for this experimental purpose appears to be PENG-D, which is explicitly intended as a replacement for RDF and OWL in the Semantic Web for use by ordinary Web authors (<cite>Schwitter 2004, 2005a, 2005b</cite>). It is purportedly easy to learn and use, particularly with a validity-checking editor (<cite>Schwitter 2003</cite>), but no empirical evidence about this claim has apparently been produced. An alternative candidate is Attempto Controlled English (derived from the same research heritage as PENG-D), which was designed as a formalism for first-order predicate logic but has recently been equipped with software that maps queries expressed in it to RDF-QL (<cite>Bernstein 2004, p. 3</cite>). So far, its RDF equivalence has been asserted only in the context of queries, not content authoring (or ontology declaration).</p>

<p>For each of the two formalisms, the experiment varies the authoring environment among four conditions. The conditions differ in the kind of support the author has in the process of representing meanings:</p>

<ol>
<li>an editing environment with printed documentation available for consultation</li>
<li>an editing environment with real-time automated monitoring and validation based on the assumption that the author is composing sequentially (e.g., Vertan 2003)</li>
<li>a direct-manipulation authoring environment with menus, tool palettes, and automated validation on request (e.g., Power 2003)</li>
<li>an interview environment that elicits representations as a sequence of choices and free answers</li>
</ol>

<p>The four authoring environments crossed with the two formalisms comprise eight conditions, and a ninth condition is natural-language authoring in an editing environment.</p>

<p>Subjects perform a restricted authoring task. The restrictions are designed to guarantee that the RDF semantic model is sufficiently expressive to cover the meanings that are to be represented. Thus, this experiment does not attempt to test the expressiveness of the semantic model, nor does it attempt to test the ability of ordinary authors to invent work-arounds to stretch the semantic model's expressivity.</p>

<p>The task is designed not to be biased in favor of either natural-language or formal semantic representations. For this purpose, the meanings are suggested to the subjects with nonlinguistic stimuli. These may consist of physical objects, drawings, photographs, silent motion pictures, performances of music, or tasks described linguistically such that the meanings to be expressed are not represented with the same words as the task descriptions. As mentioned above, the meanings to be expressed must be restricted so they are within the domain knowledge of the subjects, or, conversely, the subjects must be selected so they have the domain knowledge required by the tasks.</p>

<p>Each subject performs multiple (tentatively 5) successive tasks, to permit the rate at which the subject improves productivity with increasing experience and the maximum attainable productivity to be estimated.</p>

<p>Subject training varies in duration, ending for each subject when that subject demonstrates on periodic tests a mastery of the formalism and authoring environment that the subject is being trained to operate with.</p>

<p>The other dependent variables include subjects' task-completion times, task performance qualities, enjoyment of the experience, and anticipated willingness to perform similar authoring in the real world.</p>

<p>The research could continue post-experimentally if the results showed that any combination of formalism and authoring environment is sufficiently competitive with natural-language authoring to make field tests or human-computation projects feasible.</p>

<h3><a id="pubs">Publications</a></h3>

<p>The pilot study can be reported in an article reporting its motivation, design, and results, and the implications for the viability of the Semantic Web visions and further useful research.</p>

<p>The main study investigates the impacts of two independent variables: the formalism and the authoring environment. Before the experiment is conducted, we do not know whether their impacts will be additive or interactive. If the impacts are largely additive, it seems reasonable to report the findings in two articles, one focused on each independent variable.</p>

<p>If the experiment's results warranted the extension of the research into the real world, additional publications would report results of field tests or human-computation activities.</p>

<h3><a id="req">Requirements</a></h3>

<p>The estimated effort required for this work, in person-weeks, is:</p>

<table>
<tr><td>Pilot study</td><td class="r">10</td></tr>
<tr><td>Drafting, submission, and revision of pilot-study article</td><td class="r">4</td></tr>
<tr><td>Review of related work</td><td class="r">4</td></tr>
<tr><td>Arrangements with collaborators</td><td class="r">3</td></tr>
<tr><td>Experimental design</td><td class="r">9</td></tr>
<tr><td>Administration of human-subjects review</td><td class="r">1</td></tr>
<tr><td>Conduct of experiment</td><td class="r">6</td></tr>
<tr><td>Analysis of experiment results</td><td class="r">2</td></tr>
<tr><td>Drafting of article 1</td><td class="r">4</td></tr>
<tr><td>Drafting of article 2</td><td class="r">3</td></tr>
<tr><td>Submission of articles</td><td class="r">1</td></tr>
<tr><td>Revision of articles</td><td class="r">2</td></tr>
<tr><td>Administrative reporting</td><td class="r">1</td></tr>
<tr><td>Management</td><td class="r">4</td></tr>
<tr><td class="c">Total</td><td class="r">54</td></tr>
</table>

<p>These estimates are based on the following assumptions:</p>

<ol>
<li>Each of the 9 experimental conditions would be performed on 20 subjects.</li>
<li>4 subjects could be participating at once.</li>
<li>Subjects using a formalism would be taught it and their authoring environment either automatically or in groups of same-treatment subjects.</li>
<li>Existing authoring environments would be available for adaptation to the experiment.</li>
</ol>

<p>Anticipated collaborators include:</p>

<ul>
<li>Stuart Weibel, Michael Crandall, and Stuart Sutton, iSchool and Dublin Core Metadata Initiative (Weibel visiting through December 2006): metadata usability</li>
<li>Scott Farrar, Linguistics (visiting Autumn 2006): ontology usability</li>
<li>Rolf Schwitter, Macquarie University, Sydney, Australia: PENG-D and its validating editor</li>
<li>Jan Spyridakis and Judith Ramey, Technical Communication: laboratory and Internet-based design of controlled-language and interface usability experiments</li>
<li>Sharon Oviatt and Phil Cohen, Oregon Health &amp; Science University: user interface design and experimentation</li>
<li>Daniel Weld, CSE: user interface design</li>
</ul>

<h3><a id="refs">References</a></h3>

<p class="h">Berners-Lee 2001. Tim Berners-Lee, James Hendler, and Ora Lassila, "The Semantic Web", <i>Scientific American</i>, 284(5), 2001, 34-43. <a href="http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&amp;catID=2">http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&amp;catID=2</a>.</p>

<p class="h">Bernstein 2004. Abraham Bernstein, Esther Kaufmann, Norbert E. Fuchs, and June von Bonin, "Talking to the Semantic Web--A Controlled English Query Interface for Ontologies", delivered at 14th Workshop on Information Technology and Systems, 2004. <a href="http://www.ifi.unizh.ch/ddis/staff/goehring/btw/files/BernsteinEtAl_WITS2004.pdf">http://www.ifi.unizh.ch/ddis/staff/goehring/btw/files/BernsteinEtAl_WITS2004.pdf</a>.</p>

<p class="h">Clark 2005. Peter Clark, Phil Harrison, Tom Jenkins, John Thompson, and Rick Wojcik, "Acquiring and Using World Knowledge Using a Restricted Subset of English", 2005. <a href="http://www.cs.utexas.edu/users/pclark/papers/flairs.pdf">http://www.cs.utexas.edu/users/pclark/papers/flairs.pdf</a>.</p>

<p class="h">Dean 2003. Rebecca J. Dean, "FAST: Development of Simplified Headings for Metadata", presented at Authority Control: Definition and International Experiences, 2003. <a href="http://www.sba.unifi.it/ac/relazioni/dean_eng.pdf">http://www.sba.unifi.it/ac/relazioni/dean_eng.pdf</a>.</p>

<p class="h">Marshall 2003. Catherine C. Marshall and Frank M. Shipman, "Which Semantic Web?", <i>Hypertext '03 Proceedings</i>, 2003. <a href="http://www.csdl.tamu.edu/~marshall/ht03-sw-4.pdf">http://www.csdl.tamu.edu/~marshall/ht03-sw-4.pdf</a>.</p>

<p class="h">Mitamura 2003. Teruko Mitamura, Kathryn Baker, David Svoboda, and Eric Nyberg, "Source Language Diagnostics for MT", <i>Proceedings of MT Summit IX</i>, 2003. <a href="http://www.lti.cs.cmu.edu/Research/Kant/PDF/mtsummit03-camera.pdf">http://www.lti.cs.cmu.edu/Research/Kant/PDF/mtsummit03-camera.pdf</a>.</p>

<p class="h">M&oslash;ller 2003. Margrethe H. M&oslash;ller, "Grammatical Metaphor, Controlled Language and Machine Translation", <i>EAMT-CLAW 2003</i>, 2003. <a href="http://www.mt-archive.info/CLT-2003-Moeller.pdf">http://www.mt-archive.info/CLT-2003-Moeller.pdf</a>.</p>

<p class="h">Power 2003. Richard Power, Donia Scott, and Anthony Hartley, "Multilingual Generation of Controlled Languages", <i>EAMT/CLAW 2003</i>, 2003. <a href="http://www.mt-archive.info/CLT-2003-Power.pdf">http://www.mt-archive.info/CLT-2003-Power.pdf</a>.</p>

<p class="h">Schwitter 2003. Rolf Schwitter, Anna Ljungberg, and David Hood, "ECOLE: A Look-ahead Editor for a Controlled Language", <i>Controlled Translation, Proceedings of EAMT-CLAW03</i>, 2003. <a href="http://www.ics.mq.edu.au/~rolfs/papers/CLAW03-ECOLE.pdf">http://www.ics.mq.edu.au/~rolfs/papers/CLAW03-ECOLE.pdf</a>.</p>

<p class="h">Schwitter 2004. Rolf Schwitter and Marc Tilbrook, "Controlled Natural Language meets the Semantic Web", <i>Proceedings of the Australasian Language Technology Workshop 2004</i>, 2004. <a href="http://www.ics.mq.edu.au/~rolfs/papers/alta04-schwitter-tilbrook.pdf">http://www.ics.mq.edu.au/~rolfs/papers/alta04-schwitter-tilbrook.pdf</a>.</p>

<p class="h">Schwitter 2005a. Rolf Schwitter, "A Controlled Natural Language Layer for the Semantic Web", <i>AI 2005, Advances in Artificial Intelligence: 18th Australian Joint Conference on Artificial Intelligence</i>, 2005. <a href="http://www.ics.mq.edu.au/~rolfs/papers/ai2005-sydney.pdf">http://www.ics.mq.edu.au/~rolfs/papers/ai2005-sydney.pdf</a>.</p>

<p class="h">Schwitter 2005b. Rolf Schwitter, "Controlled Natural Language as Interface Language to the Semantic Web", <i>Proceedings of the 2nd Indian International Conference on Artificial Intelligence (IICAI-05)</i>, 2005. <a href="http://www.ics.mq.edu.au/~rolfs/papers/IICAI-schwitter-2005.pdf">http://www.ics.mq.edu.au/~rolfs/papers/IICAI-schwitter-2005.pdf</a>.</p>

<p class="h">Shirky 2003. Clay Shirky, "The Semantic Web, Syllogism, and Worldview", 2003. <a href="http://www.shirky.com/writings/semantic_syllogism.html">http://www.shirky.com/writings/semantic_syllogism.html</a>.</p>

<p class="h">Vertan 2003. Cristina Vertan and Walther v. Hahn, "Menu Choice Translation: A Flexible Menu-Based Controlled Natural Language System", delivered at EAMT/CLAW 2003, 2003. <a href="http://www.eamt.org/archive/dublin/Vertan_VonHahn.pdf">http://www.eamt.org/archive/dublin/Vertan_VonHahn.pdf</a>.</p>

<p class="h">Vizine-Goetz 2002. Diane Vizine-Goetz, "Classification Schemes for Internet Resources Revisited", <i>Journal of Internet Cataloging</i> 5 (4), 2002. <a href="http://polaris.gseis.ucla.edu/jfurner/02-03/462/vizine.doc">http://polaris.gseis.ucla.edu/jfurner/02-03/462/vizine.doc</a>.</p>

<p class="c"><a href="http://validator.w3.org/check?uri=referer">
<img
	style="border: 0; margin: 3px"
	src="http://www.w3.org/Icons/valid-xhtml11"
	alt="Valid XHTML 1.1!"
	height="31"
	width="88"
/>
</a></p>

</body>
</html>
