<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

<title>Translators in a Global Community</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="styles.css" title="styles" />

</head>

<body style="margin: 30pt; background-color: #EEEEDD">

<h2>Translators in a Global Community</h2>

<h3>Old title: Panlingual Translation and Panhuman Culture</h3>

<h3><b>Jonathan Pool</b></h3>
<h4 class="c">Turing Center, University of Washington<br />
and<br />
Utilika Foundation</h4>

<h4>Contents</h4>
<p class="l"><a href="#abs">Abstract</a><br />
<a href="#prob">Translation as a Human Right</a><br />
<a href="#tac">Translation Versus Extinction</a><br />
<a href="#strat">Making Translation Panlingual</a><br />
<a href="#conc">Panlingual Culture</a><br />
<a href="#refs">References</a></p>

<h3><a name="abs">Abstract</a></h3>

<h4>Version 5 (Pool)</h4>

<p>What would translators do in a global community if thousands of natural languages remained alive? People would require <i>panlingual</i> translation, mediating among <i>all</i> cultures. Impractical? Perhaps not. Suppose (1) the global community had a global culture and a compatible interlingua, and (2) each of the world's natural languages had a "universal" variety formally equivalent to the interlingua. Then panlingual translation could be a process of human-machine collaboration. Humans would translate <i>interculturally</i> (between traditional and universal varieties), while machines translated <i>interlingually</i> (among all universal varieties). Thus, a work translated from language A into language B would pass through two human translators and two machine translators. A biliterate (fluent in both traditional and universal written varieties) could obviate one of the two human translators by writing or reading directly in a universal variety. Traditional translation could coexist with collaborative panlingual translation, producing distinct results (e.g., more authentic, less auditable). Elements of this concept can be simulated for empirical estimation of its feasibility, efficacy, stability, and popularity.</p>

<h4>Version 4 (Pool)</h4>

<p>What would translators do in a global community? If thousands of natural languages remained alive, people would require <i>panlingual</i> translation, mediating among <i>all</i> cultures. Impractical? Perhaps not. Suppose (1) the global community has a global culture and a compatible interlingua, and (2) each of the world's natural languages has a "universal" variety that is formally equivalent to the interlingua. Then panlingual translation can be a process of human-machine collaboration. Humans can translate <i>intralingually</i> between traditional and universal varieties, while machines translate <i>interlingually</i> among all universal varieties. Thus, a work translated from language A into language B passes through two human translators and two machine translators. A biliterate (fluent in both traditional and universal written varieties) can replace one of the two human translators by writing or reading directly in a universal variety. Traditional translation can coexist with collaborative panlingual translation, producing distinct results (e.g., more authentic, less auditable). Elements of this concept can be simulated for empirical estimation of its feasibility, efficacy, stability, and popularity.</p>

<h4>Version 3 (Colowick)</h4>

<p>What would human translators do in a truly global community? If the community respected the equality [valued the viability?] of thousands of natural languages, its members would require panlingual translation, which would mediate among all cultures. One proposed model assumes that a global community has a global culture and a compatible global interlingua. In this model, each of the world's natural languages has a "universal" variety that is formally equivalent to the interlingua. The process of panlingual translation is then one of human-machine collaboration: humans translate intralingually between traditional and universal varieties, while machines translate interlingually among all universal varieties. Thus, a work translated from language A into language B has passed through two human translators as well as two machine translators. Elements of this model can be simulated for empirical estimation of its feasibility, efficacy, stability, and popularity.</p>

<h4>Version 2 (Pool)</h4>

<p>What would translators do in a global community? If the community respected the equality of thousands of natural languages, its members would require panlingual translation, which would mediate among all cultures. Nobody could do such translation, but such a community might still be feasible. One abstract solution concept assumes that a global community has a global culture and a compatible global interlingua. Each of the world's natural languages, further, has a "universal" variety that is formally equivalent to the interlingua. The process of panlingual translation is one of human-machine collaboration. Humans translate intralingually between traditional and universal varieties, while machines translate interlingually among all universal varieties. Thus, a work translated from language A into language B has passed through two human translators as well as two machine translators. Biliterates (those fluent in their traditional and universal varieties), however, can be their own human translators. Traditional single-human bilingual translation can coexist with such collaborative panlingual translation, producing distinct results (e.g., more authentic, less auditable). Elements of this concept can be simulated for empirical estimation of its feasibility, efficacy, stability, and popularity.</p>

<h4>Version 1 (Colowick)</h4>

<p>Widely cherished ideals of diversity and community may be realized in an environment of universal interactivity. The two interdependent components of this environment would be panlingual translation (the ability to translate among all languages) and panhuman culture (those aspects of culture that are universally shared). Arguably, panlingual translation would be an extreme of diversity, and panhuman culture would be an extreme of uniformity. As I model the problem, panlingual translation requires fully automatic translation, which requires equivalence between languages, which requires the formalization of "controlled natural" languages; and the equivalent formalization of all natural languages requires panhuman culture. Conversely, panhuman culture requires universally transcultural learning, interaction, accommodation, and negotiation, all of which require panlingual translation. Given their mutual dependence, planning for panlingual translation and panhuman culture requires overcoming a stalemate, by capitalizing on those elements of each that are possible with the already existing elements of the other.</p>

<h4>Version 0 (Pool)</h4>

<p>Panlingual translation and panhuman culture are rarely advocated, but without them, arguably, widely cherished ideals of diversity and community cannot be realized. So, are they possible? Panlingual translation would be an extreme of diversity, and panhuman culture would be an extreme of uniformity. Nonetheless, it is plausible to consider them mutually dependent, each requiring the other. As I model the problem, panlingual translation requires fully automatic translation, which requires equivalence between languages, which requires the formalization of "controlled natural" languages; and the equivalent formalization of all natural languages requires panhuman culture. Conversely, panhuman culture requires universally transcultural learning, interaction, accommodation, and negotiation, all of which require panlingual translation. Given their mutual dependence, planning for panlingual translation and panhuman culture requires overcoming a stalemate, by realizing those elements of each that are possible with the already existing elements of the other.</p>

<h3><a name="prob">The Problem</a></h3>

<p>"We have to appreciate the diverse cultures on the planet. But that should not be used as an obstacle to developing a new culture, and that is the point. There is a new culture developing. It's a shared culture. It incorporates the best of Western and Eastern and Asian and African culture, and it is part of the new planetary civilization of the future." Paul Kurtz, interviewed on "Point of Inquiry" (radio broadcast), 12 May 2006.</p>

<p>Claimed universal information rights, such as rights to education and public disclosures, arguably imply rights to translation into all living languages. Moreover, those languages may not survive unless they are useful for global communication and are thus sources and targets of translation. So, why not translate everything into every language on demand? Panlingual translation could be efficient if it were fully automatic, but the quality of automatic translation is low. One solution is to adopt a universal semantic standard, use it to define semantically equivalent "universal" varieties of all languages, and then implement automatic translation among the universal varieties. The negotiation of the standard would be a cultural compromise. Authors and professional translators would collaborate as mediators between their native cultures and the universal one. Current efforts to negotiate universal concept taxonomies (ontologies, thesauri, subject-heading systems, etc.) with multilingual terms illustrate the possibilities of and obstacles to this solution.</p>

<p>Situations of language contact have often produced language assimilation and language death. Although there are reportedly about 7,000 natural languages in the world (Gordon 2005), the contacts that occur amidst contemporary globalization will, according to several forecasts, make most of these languages extinct within a century (Woodbury 2006).</p>

<p>In order to preserve or restore the vitality of weak languages without enforcing the isolation of their speech communities, it appears necessary to make the use of weak languages a rational choice for their speakers. Mufwene (2002) argues that the proximate cause of most language death is the self-interested decisions made by the dying languages' own speakers to cease using them, and the only promising interventions to stop or reverse language death are ones that confer rewards on the users of weak languages. There is evidence that their speakers value them and would maintain them if it were "possible for speakers to earn their living competitively in these languages" (Mufwene 2002, 390).</p>

<p>Designing an intervention to facilitate global interactivity while preserving linguistic diversity (a condition that I shall abbreviate as "panlingual interactivity") seems difficult in light of this analysis. To succeed, it must apparently allow and motivate people to use their native languages as languages of productivity. If expressions' meanings can be reliably translated by automatic means from any language into any other language, they can be encoded in any language without losing value. But otherwise their value will be greater if they are encoded in a widely known language.</p>

<p>The problem of making expressions translatable among arbitrary natural languages is difficult in part because of discordant lexical ambiguity, in which an ambiguous lexeme in one language does not correspond to a lexeme with exactly the same set of denotations in every other language. Efficient panlingual interactivity seems to depend on systems that can reliably, automatically, and fully resolve discordant lexical ambiguity in any natural language. This dependence is problematic, because no system even tries to do this.</p>

<h3><a name="tac">Solution Tactics</a></h3>

<p>If there is a strategy that could make panlingual interactivity practical, we might find tactics for it in related existing projects. I consider five existing ones below.</p>

<h4>Plone</h4>

<p>Plone (<a href="http://plone.org">http://plone.org</a>) is "a content management system with strong multilingual support". Its functionalities include Web and portal services, document publishing, and groupware services. A typical application is to operate a Web site whose content can be displayed in any of several languages.</p>

<p>In principle, Plone permits any content to be transmitted to a user in any language, and it permits content elements of a site that appear in multiple contexts to be translated once and reused. Plone's tactics include making language a parameter that can be applied to minimal units of language-dependent content and using negotiation between the server and the client to set that parameter.</p>

<p>A final Plone tactic is to make its own development interface multilingual. The site developer sees Plone, as the site end-user sees the site, in whichever language is preferred. Plone's development interface currently exists in about 50 languages.</p>

<h4>Unicode</h4>

<p>Unicode (<a href="http://www.unicode.org/">http://www.unicode.org/</a>) is a standardization project for the adoption of a single text encoding capable of representing texts in all languages, natural and artificial. Unicode makes provision for about 1 million characters. Thus, Unicode has made the character space large enough so the standard can be effectively panlingual. Unicode's abundant character space makes possible another tactic: decentralized initiative. Decisions on characters in one script do not interact with decisions on characters in another script, so experts on particular scripts can autonomously formulate proposals for the inclusion of their scripts. A third Unicode tactic is to make decisions by collaborative majoritarian opinion aggregation.</p>

<p>Unicode began explicitly in order to be panlingual. It is still moving toward panlinguality, particularly with respect to ancient scripts, but being designed for panlinguality it does not need to be rearchitected as its coverage expands.</p>

<h4>WordNet</h4>

<p>WordNet (<a href="http://wordnet.princeton.edu/">http://wordnet.princeton.edu/</a>) is an activity that identifies word senses and associates them with lexemes. Its main tactic is to compile and maintain a knowledge base that identifies senses of content lexemes (more specifically, noun, verb, adjective, and adverb words and collocations). This work is performed by experts who use judgment in ascertaining the senses that deserve to be distinguished, the glosses that explain their distinct meanings, and the lexeme-sense associations. The work has three relevant results. First, it provides a collection of word senses ("synsets"). Second, it makes it possible to determine for any sense which lexemes can express it, and for any lexeme which senses it can express. Third, it assigns to senses particular properties (such as verb frame, category, and usage) and relations (such as subtype-supertype, component, member, ingredient, pertinence, and attribute). These results are language-specific.</p>

<p>A second WordNet tactic is the replication of WordNets for multiple languages. WordNet began as a unilingual project, dealing only with United States English. Different initiators subsequently compiled WordNets for additional languages. There are now 47 WordNets, including up to 3 different ones for the same language and one trilingual WordNet, covering 39 languages.</p>

<p>A third WordNet tactic is the translingual association of word senses. This work is conducted by the Global WordNet Association (<a href="http://www.globalwordnet.org/">http://www.globalwordnet.org/</a>), EuroWordNet (<a href="http://www.illc.uva.nl/EuroWordNet/">http://www.illc.uva.nl/EuroWordNet/</a>), MultiWordNet (<a href="http://multiwordnet.itc.it/english/home.php">http://multiwordnet.itc.it/english/home.php</a>), and Mimida (<a href="http://www.gittens.nl/SemanticNetworks.html">http://www.gittens.nl/SemanticNetworks.html</a>) projects. The details of this tactic differ among the four projects. In general, associating senses translingually requires additional expert judgment as to whether some sense in one language is equivalent to some sense in another language. In general, this judgment in these projects is influenced more by the United States English version of WordNet than by any other-language versions. In the Global WordNet Association project a set of senses, called "Base Concepts", is selected according to both the United States English version of WordNet and universality, with "Global Base Concepts" being those "that act as Base Concepts in all languages of the world".</p>

<h4>Grammar Matrix</h4>

<p>The Grammar Matrix (<a href="http://www.delph-in.net/matrix/">http://www.delph-in.net/matrix/</a>) is a project for the construction of a system that automatically generates an initial computational parsing and generating grammar for any human language, given a lexicon and that language's values on a set of parameters. What makes it possible to produce an initial grammar for a language on the basis of parameter specifications is the knowledge of linguistic universals and cross-linguistic typology contained in the Grammar Matrix kernel and modules.</p>

<p>The tactic employed for eliciting the lexicon and paramater values is to present a Web form (<a href="http://www.delph-in.net/matrix/modules.html">http://www.delph-in.net/matrix/modules.html</a>) to a person who knows both the target language and any of the languages in which the form is available. The form asks questions answerable by a person who is fluent in the language. The parameters cover basic grammatical differences among the world's languages, including subject-verb-object word order, how sentences are negated, how declarative sentences are converted to yes-no questions, how two or more phrases are coordinated with the equivalent of "and", how modes such as "can" and "must" are expressed, which cases exist, how nouns are marked as indefinite ("a") or definite ("the"), how nouns are pluralized, what word categories must agree with each other, and how agreement is expressed.</p>

<p>The Grammar Matrix uses semantic representation (specifically, Minimal Recursion Semantics) as its tactic for linking the parsing of sentences with the generation of sentences and for translingual equivalence. If multiple languages' lexicons are identified (e.g., "dogfood" in English is formally named and typed the same as "aliments pour chiens" in French), then one grammar can generate surface forms from the semantic representation that another grammar produces in parsing, thereby translating between the languages.</p>

<p>The expert who provides the lexicon and the parameter values to the Grammar Matrix thereby defines the variety of a language for which a grammar will be produced, and is therefore in control over the ambiguity in that variety. By specifying a constrained morphology and syntax and a word-sense-differentiated lexicon, the expert can force the Grammar Matrix to generate a grammar for a "controlled" variety of the language, one that finds and generates no lexical or structural ambiguity.</p>

<h4>Semantic Web</h4>

<p>The "Semantic Web" initiative (Berners-Lee 2001) envisions a network platform for universal interactivity. Although various flavors of this initiative have been identified (Marshall 2003), the most widely advocated flavor calls for a universalization of the World Wide Web's producing and consuming publics, to include not only human beings everywhere but also computers and physical devices.</p>

<p>The main enabling tactic advocated in the Semantic Web initiative is the semantic formalization of Web content. This formalization takes two forms. First, statements made in Web documents are to comply with a formal syntax. Second, references within statements are to be expressly defined.</p>

<p>Decentralized authority applies to definitions in the Semantic Web. Authors wishing to use a term must cite a definition of it, but they are free to cite any definition of their choosing. They may cite an already-published definition, or they may publish their own definition and cite that. Nonetheless, the Semantic Web envisions a tactic of re-using consensual definitions and a practice of making definitions rich. If this consensual tactic were implemented thoroughly, then whenever a Web document characterized anything as "dogfood" in a particular sense then it would cite the same definition. This would permit services that inspect the Web's content to combine content from different authors correctly. The richness of definitions would be achieved by the organization of definitions into ontologies, which provide not merely natural-language explications of meanings but also formal constraints and entailments, such as value types, transitivity, and exclusivity. The use of shared ontologies would permit automated reasoning from Web content, deriving conclusions not expressly stated (such as using information about dogfood to answer questions about pet food).</p>

<h3><a name="strat">A Vision</a></h3>

<p>We can combine some of the tactics described above to design a strategic vision for panlingual interactivity. In particular, I propose to combine:</p>

<ul>
<li>panlinguality by design, from Unicode</li>
<li>a multilingual development interface, from Plone</li>
<li>syntactic formalization, from the Semantic Web</li>
<li>translingual lexical standardization, from WordNet</li>
<li>ontological referencing, from the Semantic Web</li>
<li>natural-language-based semantic representation, from the Grammar Matrix</li>
</ul>

<p>The proposed strategy would support panlingual interactivity with what I'll call "collaborative panlingual semantic standardization". For brevity, I'll call the strategy "CPSS". CPSS as I imagine it can be considered to be a component of the Semantic Web, providing functionalities inspired by the other four projects.</p>

<p>CPSS would make Semantic Web panlinguality practical. Without CPSS, the Semantic Web would permit ordinary Web users to construct their own ontologies and semantically annotated content. This suggests content creators would not use others' existing ontologies unless they could understand them, which in most cases they couldn't because each ontology author would provide glosses only in the language the author knew. The result would be an explosion of synonymity even more powerful than forecast by Marshall (2003), where panlinguality wasn't under consideration. The synonymity, however, would be imperfect, because ontology authors would be influenced by the lexeme-sense mappings of their own languages. So, ontology matching would be error-laden, and panlingual search and knowledge extraction would suffer. Even when multiple definitions were equivalent, there would be no way to guarantee the automatic discovery of the equivalence, and, even if the equivalence were discovered, the effort in formulating multiple equivalent definitions would be wasteful.</p>

<p>While lexical standardization is optional in the Semantic Web vision, it would be essential in CPSS, and this would require that the community of CPSS adopters be adequately motivated to share ontologies. The cost of sharing for each adopter would be foregoing the ability to define concepts exactly as the adopter prefers, and to forego in part concepts that conform maximally to lexeme senses in the adopter's main language. Moderating that cost, CPSS would permit adopters to participate equitably and efficiently in the construction of the shared ontologies. The concepts in the ontologies would not be discovered, as in principle the synsets of (at least some) WordNets are, but would be chosen collaboratively, with deliberation and bargaining, by the CPSS community. Should an ontology contain a concept expressed by the English verb "to dogfood"? The community's decision process would answer that, and no single language's word-sense inventory would be the presumed determinant of the concept inventory.</p>

<p>Syntactic standardization, unlike lexical standardization, is mandatory in the Semantic Web vision, but CPSS would help make it panlingually accessible to the user community. One of the most plausible doubts about the Semantic Web vision is that its cognitive demands on users would be intolerable. Marshall (2003) argues that no user-friendly authoring interface could spare users the need to make their knowledge, intentions, and presuppositions precise and that this requirement is equivalent to the training that turns a person into a knowledge engineer. If there is any escape from this burden, it seems likely to lie in the use of native-language intuitions to help users reason correctly about ontological design and use. CPSS would use the Grammar Matrix approach to permit native speakers of the world's languages to define morphosyntactic fragments of their languages that can unambiguously generate at least semantic representations equivalent to those that the Semantic Web's OWL, RDF Schema, and RDF can express. CPSS would make use of knowledge about controlled-language expressivity and ambiguity (Pool 2005), knowledge from various related unilingual controlled-language projects (e.g., Clark 2005, Kaljurand 2006, Sowa 2004), and knowledge about the efficiency of constrained communication (Ford 1979).</p>

<p>CPSS would make maximal use of dogfooding in order to panlingualize not only its products but also its process. Other multilingual projects, such as Plone, pursue this approach to some extent, but stop after permitting consumer users and individual site creators to work in multiple languages. CPSS would be more thorough about this. It would not rely on the assumption that every activist (e.g., policymaking participant) in the adopter community shares a common natural language (such as English). Its aim would be to facilitate participation in all aspects of the activity by persons with relevant knowledge and skill, regardless of their native languages and regardless of which additional languages, if any, they know. The Grammar Matrix lexicon-input and parameterization form would be a CPSS-compliant document, so as soon as it has been completed by a bilingual user it would become available in the language for which it has just been completed. This would permit monolingual speakers of the language to inspect and question the choices made, and permit bilinguals who know that language to use the form in that language for the generation of grammars for additional languages. The voting process in which members of the community adopt ontology decisions would likewise have an interface made from CPSS-compliant forms, permitting participation via any CPSS-enabled language. The deliberative process would be a greater dogfooding challenge, but one from which it would not be reasonable for CPSS to retreat, since deliberation is an activity type that those envisioning a universally accessible Web want to include in its scope. More than the other activities, the dogfooding of deliberation would validly test the expressivity achieved by CPSS as it panlingually develops.</p>

<h3><a name="conc">Conclusion</a></h3>

<p>The strategic vision described here, if attempted, might succeed or fail, but in either case it would be a realistic test of the feasibility of the mass-participation element of the proposed Semantic Web. It is unreasonable to debate the merits of the Semantic Web idea without asking it to deal explicitly with the existence of 7,000 languages in the world. An attempt to implement the CPSS vision would provide empirical measures of the cost and limits of doing so.</p>

<p>Were CPSS to be practical, it would make foreseeable improvements in information retrieval, information extraction, question answering, summarization, transaction processing, deliberation, discussion, and publication. In addition, it would change the world's economy of language. Languages that are being treated as doomed would become useful for participation in global information exchange, and this would increase the value of using them productively and in other ways. The value of creating writing systems for as-yet unwritten languages, and the value of becoming literate in written minority languages, would also increase greatly. The global linguistic equilibrium could significantly change, as it became less costly to satisfy the preference for linguistic diversity and linguistic preservation.</p>

<h3><a name="refs">References</a></h3>

<p>Berners-Lee 2001. Tim Berners-Lee, James Hendler, and Ora Lassila, "The Semantic Web", <i>Scientific American</i>, 284(5), 2001, 34-43. <a href="http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&catID=2">http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&catID=2</a></p>

<p>Clark 2005. "Acquiring and Using World Knowledge Using a Restricted Subset of English", 2005. <a href="http://www.cs.utexas.edu/users/pclark/papers/flairs.pdf">http://www.cs.utexas.edu/users/pclark/papers/flairs.pdf</a>.</p>

<p>Ford 1979. W. Randolph Ford, Alphonse Chapanis, and Gerald D. Weeks, "Self-Limited and Unlimited Word Usage during Problem Solving in Two Telecommunication Modes", <i>Journal of Psycholinguistic Research</i>, 8, 1979, 451-475.</p>

<p>Gordon 2005. Raymond G. Gordon, Jr. (ed.), <i>Ethnologue: Languages of the World</i>, 15th edn. Dallas: SIL International, 2005. <a href="http://www.ethnologue.com/">http://www.ethnologue.com/</a>.</p>

<p>Kaljurand 2006. Kaarel Kaljurand and Norbert E. Fuchs, "Bidirectional Mapping between OWL DL and Attempto Controlled English", delivered at Fourth Workshop on Principles and Practice of Semantic Web Reasoning, Budva, Monte Negro, 2006. <a href="http://www.ifi.unizh.ch/attempto/publications/papers/ppswr2006_kaljurand.pdf">http://www.ifi.unizh.ch/attempto/publications/papers/ppswr2006_kaljurand.pdf</a>.</p>

<p>Marshall 2003. Catherine C. Marshall and Frank M. Shipman, "Which Semantic Web?", <i>Hypertext '03 Proceedings</i>, 2003. <a href="http://www.csdl.tamu.edu/~marshall/ht03-sw-4.pdf">http://www.csdl.tamu.edu/~marshall/ht03-sw-4.pdf</a>.</p>

<p>Mufwene 2002. Salikoko S. Mufwene, "Colonization, Globalization and the Plight of 
'Weak' Languages", <i>Journal of Linguistics</i>, 38, 2002, 375-395.</p>

<p>Pool 2005. Jonathan Pool, "Can Controlled Languages Scale to the Web?", manuscript. <a href="http://utilika.org/pubs/etc/ambigcl/">http://utilika.org/pubs/etc/ambigcl/</a>.</p>

<p>Sowa 2004. John F. Sowa, "Common Logic Controlled English", 2004. <a href="http://www.jfsowa.com/clce/specs.htm">http://www.jfsowa.com/clce/specs.htm</a>.</p>

<p>Woodbury 2006. Anthony C. Woodbury, "What is an Endangered Language?". Linguistic Society of America, 2006. <a href="http://www.lsadc.org/info/pdf_files/Endangered_Languages.pdf">http://www.lsadc.org/info/pdf_files/Endangered_Languages.pdf</a>.</p>

</body>
</html>
