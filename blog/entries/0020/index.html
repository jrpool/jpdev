    <!DOCTYPE HTML>
    <html lang="en-US">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="author" content="Jonathan Robert Pool">
        <meta name="creator" content="Jonathan Robert Pool">
        <meta name="publisher" name="Jonathan Robert Pool">
        <meta name="description" content="measuring digital accessibility">
        <meta name="keywords" content="accessibility a11y web testing">
        <title>Measuring digital accessibility with Autotest</title>
        <link rel="stylesheet" href="../../../../site/style.css">
      </head>
      <body>
        <main>
          <header>
            <p class="home"><a href="../../index.html">R&amp;D notes</a></p>
            <h1>Measuring digital accessibility with Autotest</h1>
            <p class="author">Jonathan Robert Pool</p>
            <p class="summary">Impossible, or just problematic?</p>
          </header>
          <h2>Introduction</h2>
          <p>Almost everybody wants or needs to use digital technology, and <a href="https://www.cdc.gov/ncbddd/disabilityandhealth/infographic-disability-impacts-all.html">roughly 20 to 25% of the world population has a durable disability</a>. Many more have occasional disabilities, such as bad lighting that makes it hard to distinguish colors or a rough road that makes it hard to tap a display precisely.</p>
          <p><a href="https://www.w3.org/WAI/fundamentals/accessibility-intro/#what">Web accessibility</a>, or more generally digital accessibility, comes to the rescue in these situations. It is similar to physical accessibility. Just as buildings can be equipped with ramps and wide doors to accommodate wheelchair users, similarly websites, mobile apps, email messages, and digital files can be designed and built to limit or eliminate barriers, accommodating a wide range of durable and temporary disabilities. Just as there are detailed <a href="https://www.ada.gov/regs2010/2010ADAStandards/2010ADAstandards.htm">standards for physical accessibility</a>, there are also <a href="https://www.w3.org/TR/WCAG21/">international industry standards for digital accessibility</a>. The latter are <a href="https://www.w3.org/WAI/policies/">incorporated into law in about 20 countries</a>.</p>
          <p>There is an <a href="https://www.w3.org/standards/webdesign/accessibility#case">expert consensus</a> that accessible digital products benefit not only persons with recognized disabilities, but also most other users of those products. Thus, accessibility is a closer approximation to quality in general than one might have guessed.</p>
          <p>If you care about digital accessibility, you presumably want to measure it. You want to be able to say, <q>Our main competitor&rsquo;s home page is 30% more accessible than ours</q> or <q>The latest revisions made the app even less accessible than it was before</q>. Measuring accessibility gives you a target to aim for and a tool for monitoring your progress.</p>
          <p>And, if you want to measure the accessibility of many digital artifacts at frequent intervals, automating the measurement will help. Automated measurement loses some information that human testers could provide, but gains affordability, replicability, and improvability. If you use a fully automated method to measure the accessibility of my digital product, I can apply the same method to check your results, and I can create new versions of your method to make the results more valid. It also becomes feasible to produce large-scale comparisons, such as <a href="https://disabilityhealth.jhu.edu/vaccinedashboard/webaccess/">Vaccine Website Accessibility</a>, published by Johns Hopkins University.</p>
          <h2>Tools</h2>
          <p>Products that automatically measure digital accessibility are sold or licensed for up to hundreds of thousands of dollars. But there are also open-source, free, and nearly free automatic measuring tools available for anybody to use. Three of the prominent ones are:</p>
          <ul>
            <li><a href="https://github.com/dequelabs/axe-core">axe-core</a>, created by Deque (138 tests)</li>
            <li><a href="https://github.com/IBMa/equal-access">Equal Access</a>, created by IBM (163 tests)</li>
            <li><a href="https://wave.webaim.org/">WAVE</a>, created by WebAIM (110 tests)</li>
          </ul>
          <p></p>
          <p>These tools are partly, but only partly, duplicative. Using them all provides more comprehensive measurement than using only one of them.</p>
          <p>But, even though among them these three packages perform 411 tests, much of accessibility lies beyond their reach. As one example, many users are annoyed by content that keeps blinking, flashing, or moving, and some users can suffer nausea or seizures from such content. None of the above tools is designed to report such motion.</p>
          <h2>Autotest</h2>
          <p><a href="https://github.com/jrpool/autotest">Autotest</a> is an automated testing prototype, incorporating work that began in 2018. It operates web browsers (versions of Chrome, Safari, and Firefox), following your instructions to navigate the web and perform actions on web pages. At any point in a browsing process, you can tell it what test or tests to perform. You can have it weight and aggregate the results of its tests to produce a total score for a page, site, or process. You give it all the instructions in advance; once it starts, it follows the instructions automatically.</p>
          <p>You give instructions to Autotest by writing a <dfn>script</dfn>; the documentation explains how to do this. If you are a JavaScript developer, you can also revise and extend the existing tests and scoring rules.</p>
          <p>You can define a <q>batch</q> of web pages and apply a script to all of them. Autotest will output a report for each page, and you can collect their scores to create a comparative report.</p>
          <p>There are about 30 tests currently available in Autotest. Of these, 19 are used in an accessibility-measurement procedure named <code>a11y03</code>. These tests (starting with the three test packages named above) are:</p>
          <ul>
            <li><code>axe-core</code></li>
            <li><code>Equal Access</code></li>
            <li><code>WAVE</code></li>
            <li><code>motion</code>: measures spontaneous movement on the page</li>
            <li><code>bulk</code>: measures content bloat</li>
            <li><code>zIndex</code>: measures 3-D layering</li>
            <li><code>radioSet</code>: measures nonstandard grouping of radio buttons</li>
            <li><code>role</code>: measures deprecated use of role categories</li>
            <li><code>focOp</code>: measures discrepancies in types of page content</li>
            <li><code>labClash</code>: measures violations of labeling requirements</li>
            <li><code>styleDiff</code>: measures erratic variation in item appearance</li>
            <li><code>linkUl</code>: measures difficulty of seeing links</li>
            <li><code>focInd</code>: measures ambiguity of user&rsquo;s location on page</li>
            <li><code>embAc</code>: measures click ambiguity on buttons and links</li>
            <li><code>hover</code>: measures surprises caused by hovering</li>
            <li><code>focAll</code>: measures interference with Tab-key navigation</li>
            <li><code>menuNav</code>: measures nonstandard navigation within menus</li>
            <li><code>tabNav</code>: measures nonstandard navigation within tabs</li>
            <li><code>log</code>: measures page interference with automated testing</li>
          </ul>
          <p>The <code>a11y03</code> procedure applies weights and duplication discounts to the results of these tests and generates a total <q>deficit</q> score. A score of 0 is perfect.</p>
          <p>Autotest has no opinion as to what tests best measure accessibility and how best to aggregate test results into scores. Each user can decide what instructions to give it.</p>
          <p>But the <code>a11y03</code> procedure <strong>is</strong> opinionated. It cares about conformity to prescriptive industry standards, but reaches further, attempting to estimate total accessibility. It emphasizes simplicity, stability, predictability, and fulfillment of established expectations. It also treats automated browsing as an essential prerequisite for accessibility, because (1) users with disabilities (and others) often use machines to mediate between them and the digital world, and (2) automated accessibility testing is necessary to ensure that digital products are accessible. Therefore, <code>a11y03</code> adds to the deficit scores of pages that hinder automated browsing. The total deficit scores generated by <code>a11y03</code> incorporate weights that express opinions on severity. For the three test packages, the weights are applied to the various severity levels that they report.</p>
          <p>Autotest (in various versions) has powered some web-page accessibility comparisons, including:</p>
          <ul>
            <li>New York City candidates in 2021 for <a href="https://jpdev.pro/jpdev/blog/entries/0001/index.html">mayor</a>, <a href="https://jpdev.pro/jpdev/blog/entries/0005/index.html">public advocate</a>, <a href="https://jpdev.pro/jpdev/blog/entries/0006/index.html">Manhattan district attorney</a>, and <a href="https://jpdev.pro/jpdev/blog/entries/0004/index.html">comptroller</a></li>
            <li><a href="https://jpdev.pro/jpdev/blog/entries/0008/index.html">Accessibility organizations</a></li>
            <li><a href="https://jpdev.pro/jpdev/blog/entries/0019/index.html">Human-rights organizations</a></li>
          </ul>
          <footer>
            <p class="date">Produced <time itemprop="datePublished" datetime="2021-10-31">2021/10/31</time></p>
            <p class="moddate">Revised <time itemprop="dateModified" datetime="2021-10-31">2021/10/31</time></p>
          </footer>
        </main>
      </body>
    </html>
