<!DOCTYPE HTML>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Jonathan Robert Pool">
    <meta name="creator" content="Jonathan Robert Pool">
    <meta name="publisher" name="Jonathan Robert Pool">
    <meta name="description" content="measuring digital accessibility">
    <meta name="keywords" content="accessibility a11y web testing">
    <title>Measuring digital accessibility with Autotest</title>
    <link rel="stylesheet" href="../../../../site/style.css">
  </head>
  <body>
    <main>
      <header>
        <p class="home"><a href="../../index.html">R&amp;D notes</a></p>
        <h1>Measuring digital accessibility with Autotest</h1>
        <p class="author">Jonathan Robert Pool</p>
      </header>
      <h2>Introduction</h2>
      <p>Almost everybody wants or needs to use digital technology, but <a href="https://www.cdc.gov/ncbddd/disabilityandhealth/infographic-disability-impacts-all.html">roughly 20 to 25% of the world population has a durable disability</a>. Many more have occasional disabilities, such as bad lighting that makes it hard to distinguish colors or a rough road that makes it hard to tap a display precisely.</p>
      <p><a href="https://www.w3.org/WAI/fundamentals/accessibility-intro/#what">Web accessibility</a>, or more generally digital accessibility, comes to the rescue in these situations. It is similar to physical accessibility. Just as buildings can be equipped with ramps and wide doors to accommodate wheelchair users, similarly websites, mobile apps, email messages, and digital files can be designed and built to limit or eliminate barriers, accommodating a wide range of durable and temporary disabilities. Just as there are detailed <a href="https://www.ada.gov/regs2010/2010ADAStandards/2010ADAstandards.htm">standards for physical accessibility</a>, there are also <a href="https://www.w3.org/TR/WCAG21/">international industry standards for digital accessibility</a>. The latter are <a href="https://www.w3.org/WAI/policies/">incorporated into law in about 20 countries</a>.</p>
      <p>There is an <a href="https://www.w3.org/standards/webdesign/accessibility#case">expert consensus</a> that accessible digital products benefit not only persons with recognized disabilities, but also most other users of those products. Thus, accessibility is a closer approximation to quality in general than one might have guessed.</p>
      <p>If you care about digital accessibility, you presumably want to measure it. You want to be able to say, <q>Our main competitor&rsquo;s home page is 30% more accessible than ours</q> or <q>The latest revisions made the app even less accessible than it was before</q>. Measuring accessibility gives you a target to aim for and a tool for monitoring your progress.</p>
      <p>And, if you want to measure the accessibility of many digital artifacts at frequent intervals, automating the measurement will help. Automated measurement loses some information that human testers could provide, but gains affordability, replicability, and improvability. If I use a fully automated method to measure the accessibility of your digital product, you can apply the same method to check my results, and you can create new versions of my method to make the results more valid. It also becomes feasible to produce large-scale comparisons, such as <a href="https://disabilityhealth.jhu.edu/vaccinedashboard/webaccess/">Vaccine Website Accessibility</a>, published by Johns Hopkins University.</p>
      <h2>Tools</h2>
      <p>Products that automatically measure digital accessibility are sold or licensed for up to hundreds of thousands of dollars. But there are also open-source, free, and nearly free automatic measuring tools available for anybody to use. Three of the prominent ones are:</p>
      <ul>
        <li><a href="https://github.com/dequelabs/axe-core">axe-core</a>, created by Deque (138 tests)</li>
        <li><a href="https://github.com/IBMa/equal-access">Equal Access</a>, created by IBM (163 tests)</li>
        <li><a href="https://wave.webaim.org/">WAVE</a>, created by WebAIM (110 tests)</li>
      </ul>
      <p></p>
      <p>These test packages are partly, but only partly, duplicative. Using them all provides more comprehensive measurement than using only one of them.</p>
      <p>But, even though these three packages perform a combined total of 411 tests, much of accessibility lies beyond their reach. As one example, many users are annoyed by content that keeps blinking, flashing, or moving, and some users can suffer nausea or seizures from such content. None of the above tools is designed to report such motion.</p>
      <h2>Autotest</h2>
      <p><a href="https://github.com/jrpool/autotest">Autotest</a> is an automated testing prototype, incorporating work that began in 2018. It navigates the web and performs actions on web pages as you instruct. At any point in a browsing process, you can tell it what test or tests to perform. You can have it weight and aggregate the results of its tests to produce a total score for a page, site, or process. You give it all the instructions in advance; once it starts, it follows the instructions automatically.</p>
      <p>You give instructions to Autotest by writing a <dfn>script</dfn>; the documentation explains how to do this. If you are a JavaScript developer, you can also revise and extend the existing tests and scoring rules.</p>
      <p>You can define a <q>batch</q> of web pages and apply a script to all of them. Autotest will output a report for each page, and you can collect their scores to create a comparative report.</p>
      <p>Autotest can run the three test packages named above and other custom-made tests.</p>
      <p>Current work on Autotest is focused on its procedure named <code>a11y</code>, which runs the three test packages named above and additional tests designed to supplement those packages. Version 7 of <code>a11y</code> includes these 16 additional custom tests:</p>
      <ul>
        <li><code>motion</code>: measures spontaneous movement on a page</li>
        <li><code>bulk</code>: measures content bloat</li>
        <li><code>zIndex</code>: measures 3-D layering</li>
        <li><code>radioSet</code>: measures nonstandard grouping of radio buttons</li>
        <li><code>role</code>: measures deprecated use of role categories</li>
        <li><code>focOp</code>: measures discrepancies in types of page content</li>
        <li><code>labClash</code>: measures violations of labeling standards</li>
        <li><code>styleDiff</code>: measures erratic variation in item appearance</li>
        <li><code>linkUl</code>: measures difficulty of seeing links</li>
        <li><code>focInd</code>: measures ambiguity of a user&rsquo;s location on a page</li>
        <li><code>embAc</code>: measures click ambiguity on buttons and links</li>
        <li><code>hover</code>: measures surprises caused by hovering</li>
        <li><code>focAll</code>: measures interference with Tab-key navigation</li>
        <li><code>menuNav</code>: measures nonstandard navigation within menus</li>
        <li><code>tabNav</code>: measures nonstandard navigation within tab lists</li>
        <li><code>log</code>: measures page malfunctions during automated testing</li>
      </ul>
      <p>The <code>a11y</code> procedure takes the results of the package tests and the custom tests, applies weights and duplication discounts to them, and generates a total score. A score of 0 is perfect. There is no upper limit. The more accessibility problems the tests find, and the more serious they are deemed to be, the higher the score. Scores reflect judgments of the test authors about severity. These judgments are biased toward industry standards, conventions, simplicity, stability, predictability, and testability.</p>
      <p>The tests in the <code>a11y</code> procedure make no claim to perfection. It is reasonable to consider each test failure as a warning of an accessibility risk, rather than a conclusive finding of fault. Inspection of test results reveals opportunities for improvements in tests, so the development of Autotest continues.</p>
      <p>Autotest procedures have powered some web-page accessibility comparisons, including:</p>
      <ul>
        <li>New York City candidates in 2021 for <a href="https://jpdev.pro/jpdev/blog/entries/0001/index.html">mayor</a>, <a href="https://jpdev.pro/jpdev/blog/entries/0005/index.html">public advocate</a>, <a href="https://jpdev.pro/jpdev/blog/entries/0006/index.html">Manhattan district attorney</a>, and <a href="https://jpdev.pro/jpdev/blog/entries/0004/index.html">comptroller</a></li>
        <li><a href="https://jpdev.pro/jpdev/blog/entries/0008/index.html">Accessibility organizations</a></li>
        <li><a href="https://jpdev.pro/jpdev/blog/entries/0019/index.html">Human-rights organizations</a></li>
        <li><a href="https://jpdev.pro/jpdev/blog/entries/0016/index.html">Corporations in ESG stock funds</a></li>
        <li><a href="https://jpdev.pro/jpdev/blog/entries/0022/index.html">Charitable foundations</a></li>
      </ul>
      <footer>
        <p class="date">Produced <time itemprop="datePublished" datetime="2021-10-31">2021/10/31</time></p>
        <p class="moddate">Revised <time itemprop="dateModified" datetime="2021-11-20">2021/11/20</time></p>
      </footer>
    </main>
  </body>
</html>
